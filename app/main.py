# app/main_refactored.py - –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –º–æ–¥—É–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
import sys
import os
import time
from datetime import timezone
import requests
import json
import psycopg2
import csv
from datetime import datetime, timedelta, date
from dateutil.relativedelta import *
from decimal import Decimal

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –Ω–æ–≤—ã–µ –º–æ–¥—É–ª–∏
from core.logging_utils import setup_logging
from core.kafka_utils import KafkaManager
from core.elastic_utils import ElasticsearchManager

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
health_status = 100  # ‚Üê –í–ï–†–ù–£–õ–ò –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
DB_CONFIG = {
    "host": "postgres",
    "database": "my_db", 
    "user": "user",
    "password": "password",
    "port": "5432"
}

# –ö–ª–∞—Å—Å—ã
class SetInformation:
    @staticmethod
    def set(cursor, id_value, data):
        
        to_log_file(f"\n{data[0]} | {data[1]} | {data[4]} | {data[5]}", True)
     
       
        sql_query = """INSERT INTO public.agriculture_moex(id_value, date_val, min_val, max_val, avg_val, volume, currency, date_upd)
                    VALUES(%s,
                        %s,
                        %s,
                        %s,
                        %s,
                        %s,
                        %s,
                        now()) 
                    ON CONFLICT(id_value, date_val) DO UPDATE 
                    SET min_val = EXCLUDED.min_val, 
                        max_val = EXCLUDED.max_val, 
                        avg_val = EXCLUDED.avg_val, 
                        volume = EXCLUDED.volume, 
                        currency = EXCLUDED.currency, 
                        date_upd = now()"""

        cursor.execute(sql_query, (
            id_value,
            data[1],
            data[2] if data[2] is not None else None,
            data[3] if data[3] is not None else None,
            data[4] if data[4] is not None else None,
            data[5] if data[5] is not None else None,
            data[6] if data[6] is not None else None
        ))
        
        logger.debug("Data inserted/updated in agriculture_moex", extra={
            'extra_data': {
                'event_type': 'db_data_upserted',
                'id_value': id_value,
                'date': data[1],
                'price': data[4]
            }
        })

# –§—É–Ω–∫—Ü–∏–∏
def to_log_file(str_to_log, flag_print=False):
    """–§—É–Ω–∫—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª"""
    if flag_print:
        print(str_to_log)
    path = "/app/logs"
    if not os.path.exists(path):
        os.mkdir(path)
    file_name = path + "/log_ore_futures" + str(date.today()) + ".txt"
    with open(file_name, 'a', encoding='utf-8') as file:
        file.write(str_to_log)

def get_data_json(url, contract_name):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å Singapore Exchange"""
    global health_status  # ‚Üê –î–û–ë–ê–í–ò–õ–ò –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    data = []
    try:
        logger.info(f"Fetching data for {contract_name}")
        response = requests.get(url)
        
        if response.status_code != 200:
            logger.warning(f"API returned status {response.status_code} for {contract_name}")
            return []
        
        response_data = response.text
        if not response_data:
            logger.warning(f"Empty response for {contract_name}")
            return []
            
        try:
            data1 = json.loads(response_data)
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error for {contract_name}: {e}")
            return []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞
        if 'meta' in data1:
            error_code = data1['meta'].get('code')
            error_msg = data1['meta'].get('message', 'Unknown message')
            
            if error_code != '200':
                logger.warning(f"API error for {contract_name}: {error_msg} (code: {error_code})")
                return []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        if 'data' not in data1:
            logger.warning(f"No 'data' field in response for {contract_name}")
            return []
            
        if data1['data'] is None:
            logger.warning(f"Data is None for {contract_name}")
            return []
            
        if not isinstance(data1['data'], list):
            logger.warning(f"Data is not a list for {contract_name}, type: {type(data1['data'])}")
            return []
            
        if len(data1['data']) == 0:
            logger.warning(f"Empty data list for {contract_name}")
            return []
            
        logger.info(f"Successfully retrieved {len(data1['data'])} records for {contract_name}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        for n in data1['data']:
            if 'base-date' not in n:
                logger.warning(f"Missing 'base-date' in record for {contract_name}")
                continue
                
            date_obj = datetime.strptime(n['base-date'], "%Y%m%d")
            formatted_date = date_obj.strftime("%Y-%m-%d")
            
            data.append({
                'name': contract_name,
                'daily-settlement-price-abs': n.get('daily-settlement-price-abs'),
                'total-volume': n.get('total-volume'),
                'formatted_date': formatted_date
            })
        
        return data
        
    except requests.exceptions.RequestException as er:  # ‚Üê –î–û–ë–ê–í–ò–õ–ò –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        logger.error(f'Network error for {contract_name}: {er}')
        health_status = 0
        return []
    except ValueError as err:  # ‚Üê –î–û–ë–ê–í–ò–õ–ò –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        logger.error(f'Data parsing error for {contract_name}: {err}')  
        health_status = 0
        return []
    except Exception as e:
        logger.error(f'Unexpected error for {contract_name}: {str(e)}')
        health_status = 0  # ‚Üê –û–ë–ù–û–í–õ–Ø–ï–ú —Å—Ç–∞—Ç—É—Å –ø—Ä–∏ –æ—à–∏–±–∫–µ
        return []

def insert_record_if_not_exists():
    """–í—Å—Ç–∞–≤–∫–∞ –∑–∞–ø–∏—Å–µ–π –≤ www_data_idx –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
    try:
        print("üîç DEBUG: insert_record_if_not_exists() started")
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
        name_list = []
        current_date = datetime.now() - timedelta(days=1)
        dict_month = ["F", "G", "H", "J", "K", "M", "N", "Q", "U", "V", "X", "Z"]
        for i in range(0, 37):
            next_date = current_date + relativedelta(months=i)
            name_list.append(f"FEF{dict_month[next_date.month-1]}{str(next_date.year)[-2:]}")

        print(f"üîç DEBUG: Generated {len(name_list)} contracts")

        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        inserted_count = 0
        
        for name in name_list:
            cursor.execute("SELECT id FROM public.www_data_idx WHERE name_eng = %s AND source = 'ore_futures'", (name,))
            record = cursor.fetchone()

            if record is None:
                cursor.execute("SELECT MAX(id) FROM public.www_data_idx")
                max_result = cursor.fetchone()
                max_id = max_result[0] if max_result[0] is not None else 0
                new_id = max_id + 1
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ú–µ–Ω—è–µ–º –Ω–∞ —Ä—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                cursor.execute("""
                    INSERT INTO public.www_data_idx 
                    (id, mask, name_rus, name_eng, source, url, descr, date_upd) 
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    new_id, 
                    None,  # mask
                    '–ñ–µ–ª–µ–∑–Ω–∞—è —Ä—É–¥–∞ 62% Fe',  # ‚Üê –ò–°–ü–†–ê–í–ò–õ–ò –ù–ê –†–£–°–°–ö–û–ï!
                    name,  # name_eng
                    'ore_futures',  # source
                    'https://api.sgx.com/derivatives/v1.0/history/symbol/',  # url
                    None,  # descr
                    datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # date_upd
                ))
                
                inserted_count += 1
                print(f"‚úÖ INSERTED: {name} with id {new_id}")

        conn.commit()
        cursor.close()
        conn.close()
        print(f"üîç DEBUG: Completed, inserted {inserted_count} contracts")

    except Exception as error:
        print(f"‚ùå ERROR: {error}")


def set_status_robot(id, health_status, add_text):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤ health_monitor (–∏–∑ —Å—Ç–∞—Ä–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞)"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        query = "UPDATE public.health_monitor SET date_upd = now(), health_status = %s, add_text = %s WHERE id = %s"
        cursor.execute(query, (health_status, add_text, id))
        conn.commit()
        
        logger.info("Health status updated", extra={
            'extra_data': {
                'event_type': 'health_status_updated',
                'health_status': health_status,
                'add_text': add_text
            }
        })

        cursor.close()
        conn.close()

    except Exception as e:
        logger.error(f"Error updating health status: {e}")

def sync_to_elasticsearch():
    """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å Elasticsearch"""
    try:
        logger.info("Starting Elasticsearch synchronization...")
        
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # –¢–û–õ–¨–ö–û –û–î–ò–ù –ó–ê–ü–†–û–° (—Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π)
        cursor.execute("""
            SELECT am.id_value, am.date_val, am.avg_val, am.volume, am.currency,
                   wdi.name_eng, wdi.name_rus
            FROM agriculture_moex am
            JOIN www_data_idx wdi ON am.id_value = wdi.id
            ORDER BY am.date_val, wdi.name_eng
        """)
        
        synced_count = 0
        kafka_sent_count = 0
        
        for row in cursor.fetchall():
            doc = {
                'id_value': row[0],
                'date': row[1].isoformat(),
                'price': float(row[2]) if row[2] else None,
                'volume': float(row[3]) if row[3] else None,
                'currency': row[4],
                'contract': row[5],
                'name_rus': row[6],
                'source': 'moex_sgx',
                'sync_timestamp': datetime.now().isoformat()
            }
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Elasticsearch
            es_result = es_manager.send_data(doc, 'agriculture-data')
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Kafka (–û–î–ò–ù –†–ê–ó!)
            kafka_result = kafka_manager.send_message('market-data', doc)
            if kafka_result:
                kafka_sent_count += 1
                logger.info(f"SUCCESS Data sent to Kafka: {doc.get('contract', 'Unknown')} - {doc.get('date', 'No date')}")
            else:
                logger.warning(f"FAILED to send to Kafka: {doc.get('contract', 'Unknown')}")
            
            synced_count += 1
        
        kafka_manager.flush()
        logger.info(f"SUCCESS Synchronization completed! ES: {synced_count}, Kafka: {kafka_sent_count}")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"ERROR Elasticsearch synchronization error: {e}")

def get_current_date():  # ‚Üê –î–û–ë–ê–í–ò–õ–ò —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã –¥–ª—è –ª–æ–≥–æ–≤"""
    return str(datetime.fromtimestamp(int(time.time()))) + '\n'

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    global health_status, kafka_manager, es_manager  # ‚Üê –î–û–ë–ê–í–ò–õ–ò –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    
    logger.info("Script started")
    
    try:
        to_log_file("\n\n\nSTART RUN SCRIPT\n", True)
        to_log_file(get_current_date(), True)  # ‚Üê –î–û–ë–ê–í–ò–õ–ò –∑–∞–ø–∏—Å—å –¥–∞—Ç—ã
        
        interval = '1w'
        health_status = 100  # ‚Üê –°–ë–†–ê–°–´–í–ê–ï–ú —Å—Ç–∞—Ç—É—Å –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        #insert_record_if_not_exists()
        
        
        print("üîç DEBUG: BEFORE insert_record_if_not_exists")
        insert_record_if_not_exists()
        print("üîç DEBUG: AFTER insert_record_if_not_exists")
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
        name_list = []
        current_date = datetime.now() - timedelta(days=1)
        dict_month = ["F", "G", "H", "J", "K", "M", "N", "Q", "U", "V", "X", "Z"]
        for i in range(0, 37):
            next_date = current_date + relativedelta(months=i)
            name_list.append(f"FEF{dict_month[next_date.month-1]}{str(next_date.year)[-2:]}")
        
        print(f"Generated {len(name_list)} dynamic contracts")
        
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        logger.info("Database connection established")
        to_log_file("\nConnect to DB PostgreSQL: YES!!!\n", True)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã (–∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º —Å–∫—Ä–∏–ø—Ç–µ)
        cursor.execute("SELECT id, name_eng, url FROM public.www_data_idx where source='ore_futures'")
        rows = cursor.fetchall()
        
        total_processed = 0
        successful_contracts = 0
        
        for row in rows:
            # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º —Å–∫—Ä–∏–ø—Ç–µ
            if row[1] in name_list:  # ‚Üê –í–ï–†–ù–£–õ–ò —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é!
                url = f"{row[2]}{row[1]}?days={interval}&category=futures&params=base-date%2Cbase-date%2Ctotal-volume%2Cdaily-settlement-price-abs"
                to_log_file(f"\n-----\n{url}\n", True)  # ‚Üê –í–ï–†–ù–£–õ–ò —Ñ–æ—Ä–º–∞—Ç –ª–æ–≥–∞
                
                data_points = get_data_json(url, row[1])
                
                if data_points:
                    successful_contracts += 1
                    print(f"‚úÖ Processing {len(data_points)} records for {row[1]}")
                    
                    # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ë–î
                    for data_point in data_points:
                        data = []
                        data.append(row[0])  # id
                        data.append(data_point['formatted_date'])
                        data.append(None)  # min_val
                        data.append(None)  # max_val  
                        data.append(data_point['daily-settlement-price-abs'])
                        data.append(data_point['total-volume'])
                        data.append("USD")
                        
                        try:
                            SetInformation().set(cursor, row[0], data)
                            total_processed += 1
                        except Exception as e:
                            logger.error(f"Error inserting data for {row[1]}: {e}")
                            health_status = 0
                else:
                    print(f"‚ùå No data for {row[1]}")
        
        # –ö–æ–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        conn.commit()
        cursor.close()
        conn.close()
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å Elasticsearch
        sync_to_elasticsearch()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        set_status_robot(1012, health_status, '')
        
        to_log_file("\nFINISH RUN SCRIPT\n", True)
        print(f"\nüéâ COMPLETED: Processed {total_processed} records from {successful_contracts} contracts")
        logger.info(f"Script completed: {total_processed} records from {successful_contracts} contracts")
        
    except Exception as e:
        logger.error("Error in main", extra={'error': str(e)})
        print(f"Error: {e}")
        health_status = 0
        set_status_robot(1012, health_status, '')

# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
    logger = setup_logging('main_script')
    kafka_manager = KafkaManager()
    es_manager = ElasticsearchManager()
    
    main()